{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score \n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for removing punctuation  from the data frame\n",
    "def get_punctuation_free_data_frame(data_frame):\n",
    "    string_punctuation = string.punctuation\n",
    "    for i in range(9):\n",
    "        string_punctuation += str(i)\n",
    "    for key, rows in data_frame.iterrows():\n",
    "        input_string = data_frame.loc[key, \"title\"]\n",
    "        data_frame.loc[key, \"title\"] = ''.join([i for i in input_string if i not in string_punctuation])\n",
    "    return data_frame\n",
    "\n",
    "# Function for removing the stop words from the data frame\n",
    "def get_stop_word_free_data_frame(data_frame):\n",
    "    stop_words = list(set(stopwords.words(\"english\")))\n",
    "    for key, rows in data_frame.iterrows():\n",
    "        input_string_list = data_frame.loc[key, \"title\"].split(\" \")\n",
    "        data_frame.loc[key, \"title\"] = ' '.join([i for i in input_string_list if i.lower() not in stop_words])\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "# Get the probability of each token belonging to the particular category(Language here)\n",
    "def get_category_wise_word_probabilty(data_frame):\n",
    "    tags = data_frame[\"tags\"]\n",
    "    frequency_distribution = {}\n",
    "    # Making a dictionary of tags\n",
    "    for i in tags:\n",
    "        if i not in frequency_distribution:\n",
    "            frequency_distribution[i] = {}\n",
    "\n",
    "    vocabulary = set()\n",
    "    # Getting the word count for each category (Language)\n",
    "    for key, rows in data_frame.iterrows():\n",
    "        title = rows[\"title\"].split()\n",
    "        for word in title:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.add(word)\n",
    "            if word not in frequency_distribution[rows[\"tags\"]]:\n",
    "                frequency_distribution[rows[\"tags\"]][word] = 1\n",
    "            else:\n",
    "                frequency_distribution[rows[\"tags\"]][word] += 1\n",
    "\n",
    "\n",
    "    probability_distribution = {}\n",
    "    # Computing the probability for of each word belonging to one category\n",
    "    for word in vocabulary:\n",
    "        for key, value in frequency_distribution.items():\n",
    "            if key not in probability_distribution:\n",
    "                probability_distribution[key] = {}\n",
    "            if word in frequency_distribution[key]:\n",
    "                # Laplace smoothing Add 1\n",
    "                probability_distribution[key][word] = log((frequency_distribution[key][word] + 1) / \n",
    "                    (len(frequency_distribution[key]) + len(vocabulary)))\n",
    "            else:\n",
    "                probability_distribution[key][word] = log(1 / (len(frequency_distribution[key]) + len(vocabulary)))\n",
    "    return probability_distribution, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the total number of correct prediction\n",
    "def predict(test_data_frame, probability_distribution, vocabulary):\n",
    "    correct_prediction = 0\n",
    "    \n",
    "    # Iterating over each quetsion in test data frame and computing the probabilty of the\n",
    "    # question belonging to language mentioned below\n",
    "    for key, rows in test_data_frame.iterrows():\n",
    "        final_probability = {'php': 0, 'c#': 0, 'javascript': 0, 'java': 0, 'ruby-on-rails': 0, \n",
    "                         'c++': 0, 'python': 0, 'c': 0}\n",
    "        question_token = rows[\"title\"].split()\n",
    "        # Iterating over each token of the question\n",
    "        for token in question_token:\n",
    "            if token not in vocabulary:\n",
    "                pass\n",
    "            else:\n",
    "                # If the token is present in the vocabulary then get the probabilty of the \n",
    "                # token belonging to individual langauge and add it to the final probabilty \n",
    "                # dictionary\n",
    "                for language in list(final_probability.keys()):\n",
    "                    final_probability[language] += probability_distribution[language][token]\n",
    "    \n",
    "        # Get the lanaguage which has maximum value \n",
    "        predicted_value = ''\n",
    "        max_value = float(\"-inf\")\n",
    "        for sub_key, sub_value in final_probability.items():\n",
    "            if sub_value > max_value:\n",
    "                max_value = sub_value\n",
    "                predicted_value = sub_key\n",
    "        if predicted_value == rows[\"tags\"]:\n",
    "            correct_prediction += 1\n",
    "    \n",
    "#     print(\"Naive Bayes Classifier Non Library Accuracy: \",\n",
    "#           correct_prediction/len(test_data_frame), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive bayes classifier library\n",
    "def naive_bayes_classifier_library(data_frame, test_data_frame):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_frame['title'], data_frame['tags'], random_state = 0)\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "    correct_prediction = 0\n",
    "    print(clf.predict(count_vect.transform([\"syntax for array\"])))\n",
    "#         for key, rows  in test_data_frame.iterrows():\n",
    "#         if clf.predict(count_vect.transform([\"Spring Boot: hot swapping does not work\"])) == rows[\"tags\"]:\n",
    "#             correct_prediction += 1\n",
    "#     print(\"Naive Bayes Classifier Library Accuracy: \",\n",
    "#           correct_prediction/len(test_data_frame), \"%\")\n",
    "\n",
    "# Naive bayes classifier \n",
    "def naive_bayes_classifier(data_frame):\n",
    "    probability_distribution, vocabulary  = get_category_wise_word_probabilty(data_frame)\n",
    "    test_data_frame = data_frame[:100]\n",
    "    predict(test_data_frame, probability_distribution, vocabulary)\n",
    "    naive_bayes_classifier_library(data_frame, test_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier \n",
    "def svm_classifier(stop_word_free_data):\n",
    "    title = [i for i in stop_word_free_data.title]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    title_fitted = vectorizer.fit(title)\n",
    "    title_vectors = [vectorizer.transform([i]) for i in title]\n",
    "    X_transformed = vectorizer.fit_transform(title)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transformed, stop_word_free_data.tags, test_size = 0.25)\n",
    "    svclassifier = SVC(kernel='linear')  \n",
    "    svclassifier.fit(X_train, y_train)\n",
    "    y_pred = svclassifier.predict(X_test)  \n",
    "    print(\"SVM ACCURACY:\",accuracy_score(y_test, y_pred) * 100) \n",
    "    print(confusion_matrix(y_test,y_pred))  \n",
    "    print(classification_report(y_test,y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifical Neural network \n",
    "def artificial_neural_network(data_frame, dimension_value):\n",
    "    language_number = {'php': 0, 'c#': 1, 'javascript': 2, 'java': 3, 'ruby-on-rails': 4, 'c++': 5, 'python': 6, 'c': 7}\n",
    "    # Assigning a number of each tag\n",
    "    for key, rows in data_frame.iterrows():\n",
    "        data_frame.loc[key, \"tags\"] = language_number[rows[\"tags\"]]\n",
    "    column_attribute_name = [i for i in range(dimension_value)]\n",
    "    X = data_frame[column_attribute_name]\n",
    "    Y = data_frame[\"tags\"]\n",
    "    # create model\n",
    "    Y = to_categorical(Y)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=dimension_value, activation='relu'))\n",
    "    model.add(Dense(16, activation='softmax'))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(X, Y, epochs = 100, batch_size=10)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X, Y)\n",
    "    print(model.metrics_names[1], scores[1]*100)\n",
    "    return (model.metrics_names[1], scores[1]*100)\n",
    "\n",
    "def neural_network(data_frame):\n",
    "    # Vector size\n",
    "    dimension_value = 50\n",
    "    titles = []\n",
    "    for i in range(dimension_value):\n",
    "        data_frame[i] = 0\n",
    "    \n",
    "    # Getting the word2vec model\n",
    "    for key, rows in data_frame.iterrows():\n",
    "        titles.append(data_frame.loc[key, \"title\"].split(\" \"))\n",
    "    model = Word2Vec(titles, min_count=1, size = dimension_value)\n",
    "    \n",
    "    # Getting the vector representation of the each title by using the model , getting the \n",
    "    # vector representation of it, and finally adding it\n",
    "    word2vector = []\n",
    "    for title in titles:\n",
    "        temp = []\n",
    "        for word in title:\n",
    "            temp.append(model[word])\n",
    "        word2vector.append([sum(i)/len(i) for i in zip(*temp)])\n",
    "\n",
    "\n",
    "    weight_data_frame = pd.DataFrame.from_records(word2vector)\n",
    "    weight_data_frame[\"tags\"] = data_frame[\"tags\"]\n",
    "    \n",
    "    print(artificial_neural_network(data_frame, dimension_value))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['java']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_frame = pd.read_csv(\"Final_Tags.csv\")\n",
    "    data_frame = data_frame.sample(frac = 1).reset_index(drop = True)\n",
    "    data_frame = get_punctuation_free_data_frame(data_frame)\n",
    "    data_frame = get_stop_word_free_data_frame(data_frame)\n",
    "    \n",
    "    naive_bayes_classifier(data_frame)\n",
    "#     svm_classifier(data_frame)\n",
    "#     print(data_frame)\n",
    "#     get_n_grams(data_frame)\n",
    "#     neural_network(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c']\n"
     ]
    }
   ],
   "source": [
    "naive_bayes_classifier(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
